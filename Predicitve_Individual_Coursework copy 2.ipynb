{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSIN0097 Individual Corsework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ToshaQE/Predictive-Individual\n",
    "\n",
    "Word Count : 1042"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(60000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "# To display full output in Notebook, instead of only the last result\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\"\n",
    "%autosave 60\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stock price prediction has for a long time been a subject of many debates and exhaustive research. One concept which emerged from the literature, and which has largely dominated the debate ever since is the Efficient Market Hypothesis (EMH). EMH in its strong form states that at any point in time all information regarding a particular stock is incorporated in its current price, and it is equally likely that the price will go up or down in the immediate future, making any educated attempts at its prediction obsolete. However, empirical data shows that the strong form of EMH does not hold, and in reality, it is possible to make use of openly available fundamental and technical stock indicators to predict the likely direction of its price. This is exactly what this paper will attempt to do.\n",
    "\n",
    "In particular, we will examine a number of machine learning algorithms to determine whether a hypothetical investor should hold, sell or buy a stock given a set of indicators available to him today. The outcome variable is determined using the following logic – if the stock’s price decreases by 1%  N days in the future, the investor should sell the stock today; if price goes up by 1%, the investor should buy more stock; if stock’s value remains somewhere in between, the investor should hold. The algorithms examined hence will attempt to solve a 3-class classification task.\n",
    "\n",
    "Importantly, the structure and the analysis of this paper is insipid by the work of Matloob and Khushi (2021) (https://doi.org/10.3390/asi4010017). However, we significantly extend their analysis by deriving the labels 5 days further into the future, adding new models, fine-tuning them and deriving a voting classifier.\n",
    "\n",
    "Business applications of instruments predicting stock price direction varies drastically. It can be applied by an asset fund trying to generate additional returns on its clients’ holdings or by a central banker analysing stock market’s behaviour in the immediate future to understand potential policy context. In short, any business or an institution that is involved in stock market trade might wish to employ an algorithm developed and analysed in this paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in and formatting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AAPL Close</th>\n",
       "      <th>AAPL Adj. Close</th>\n",
       "      <th>AAPL P/E (LTM)</th>\n",
       "      <th>AAPL EPS - Est High (NTM)</th>\n",
       "      <th>AAPL EPS - Est Low (NTM)</th>\n",
       "      <th>AAPL Volume</th>\n",
       "      <th>AAPL SI (%)</th>\n",
       "      <th>AAPL Vol</th>\n",
       "      <th>AAPL # Buys</th>\n",
       "      <th>...</th>\n",
       "      <th>AMGN EPS - Est Low (NTM)</th>\n",
       "      <th>AMGN Volume</th>\n",
       "      <th>AMGN SI (%)</th>\n",
       "      <th>AMGN Vol</th>\n",
       "      <th>AMGN # Buys</th>\n",
       "      <th>AMGN # Sell</th>\n",
       "      <th>AMGN # Hold</th>\n",
       "      <th>AMGN Rating</th>\n",
       "      <th>AMGN Mkt Cap</th>\n",
       "      <th>AMGN EPS - Est Avg (NTM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/2/2003</td>\n",
       "      <td>0.264285</td>\n",
       "      <td>0.226300</td>\n",
       "      <td>82.589062</td>\n",
       "      <td>0.00714</td>\n",
       "      <td>0.00232</td>\n",
       "      <td>182317016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051974</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.55</td>\n",
       "      <td>14254890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.363758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63478.08136</td>\n",
       "      <td>1.6187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/3/2003</td>\n",
       "      <td>0.266071</td>\n",
       "      <td>0.227830</td>\n",
       "      <td>83.147187</td>\n",
       "      <td>0.00714</td>\n",
       "      <td>0.00232</td>\n",
       "      <td>147975184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.052158</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.55</td>\n",
       "      <td>8311770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.346956</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63310.76282</td>\n",
       "      <td>1.6187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/6/2003</td>\n",
       "      <td>0.266071</td>\n",
       "      <td>0.227830</td>\n",
       "      <td>83.147187</td>\n",
       "      <td>0.00714</td>\n",
       "      <td>0.00214</td>\n",
       "      <td>391594000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051699</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.55</td>\n",
       "      <td>14671690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.352543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64700.79379</td>\n",
       "      <td>1.6187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/7/2003</td>\n",
       "      <td>0.265178</td>\n",
       "      <td>0.227065</td>\n",
       "      <td>82.868125</td>\n",
       "      <td>0.00714</td>\n",
       "      <td>0.00214</td>\n",
       "      <td>346265920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050062</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.55</td>\n",
       "      <td>15768390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.345180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64662.18182</td>\n",
       "      <td>1.6191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/8/2003</td>\n",
       "      <td>0.259821</td>\n",
       "      <td>0.222478</td>\n",
       "      <td>81.194062</td>\n",
       "      <td>0.00714</td>\n",
       "      <td>0.00214</td>\n",
       "      <td>230100640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.049807</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.55</td>\n",
       "      <td>12139460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.346149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62911.77244</td>\n",
       "      <td>1.6191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date  AAPL Close  AAPL Adj. Close  AAPL P/E (LTM)  \\\n",
       "0  1/2/2003    0.264285         0.226300       82.589062   \n",
       "1  1/3/2003    0.266071         0.227830       83.147187   \n",
       "2  1/6/2003    0.266071         0.227830       83.147187   \n",
       "3  1/7/2003    0.265178         0.227065       82.868125   \n",
       "4  1/8/2003    0.259821         0.222478       81.194062   \n",
       "\n",
       "   AAPL EPS - Est High (NTM)  AAPL EPS - Est Low (NTM)  AAPL Volume  \\\n",
       "0                    0.00714                   0.00232    182317016   \n",
       "1                    0.00714                   0.00232    147975184   \n",
       "2                    0.00714                   0.00214    391594000   \n",
       "3                    0.00714                   0.00214    346265920   \n",
       "4                    0.00714                   0.00214    230100640   \n",
       "\n",
       "   AAPL SI (%)  AAPL Vol  AAPL # Buys  ...  AMGN EPS - Est Low (NTM)  \\\n",
       "0          NaN  0.051974            1  ...                      1.55   \n",
       "1          NaN  0.052158            1  ...                      1.55   \n",
       "2          NaN  0.051699            1  ...                      1.55   \n",
       "3          NaN  0.050062            1  ...                      1.55   \n",
       "4          NaN  0.049807            1  ...                      1.55   \n",
       "\n",
       "   AMGN Volume  AMGN SI (%)  AMGN Vol  AMGN # Buys  AMGN # Sell  AMGN # Hold  \\\n",
       "0     14254890          NaN  0.363758          NaN          NaN          NaN   \n",
       "1      8311770          NaN  0.346956          NaN          NaN          NaN   \n",
       "2     14671690          NaN  0.352543          NaN          NaN          NaN   \n",
       "3     15768390          NaN  0.345180          NaN          NaN          NaN   \n",
       "4     12139460          NaN  0.346149          NaN          NaN          NaN   \n",
       "\n",
       "   AMGN Rating  AMGN Mkt Cap  AMGN EPS - Est Avg (NTM)  \n",
       "0          NaN   63478.08136                    1.6187  \n",
       "1          NaN   63310.76282                    1.6187  \n",
       "2          NaN   64700.79379                    1.6187  \n",
       "3          NaN   64662.18182                    1.6191  \n",
       "4          NaN   62911.77244                    1.6191  \n",
       "\n",
       "[5 rows x 770 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"55_Firms_2.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used Koyfin platform (https://koyfin.com) to download the data. The data contains stocks closing and adjusted prices, four fundamental and technical indicators, as well as analyst recommendations for 55 firms listed on top of the S&P 500 index (as of 01/03/2022) for the period from 02 Jan 2003 to 02 Dec 2019 and contains 4260 rows. \n",
    "Tickers for the first ten companies can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'FB', 'JPM', 'UNH', 'JNJ', 'PG', 'V']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making a list of unique companies\n",
    "all_collumns = list(data.columns)\n",
    "all_tickers = []\n",
    "counter = 15\n",
    "\n",
    "for i in all_collumns[1:]:\n",
    "    if counter%14 == True:\n",
    "        all_tickers.append(i.split(\" \", 1)[0])\n",
    "    counter += 1\n",
    "\n",
    "len(all_tickers)\n",
    "all_tickers[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 14 original features are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Close',\n",
       " 'Adj. Close',\n",
       " 'P/E (LTM)',\n",
       " 'EPS - Est High (NTM)',\n",
       " 'EPS - Est Low (NTM)',\n",
       " 'Volume',\n",
       " 'SI (%)',\n",
       " 'Vol',\n",
       " '# Buys',\n",
       " '# Sell',\n",
       " '# Hold',\n",
       " 'Rating',\n",
       " 'Mkt Cap',\n",
       " 'EPS - Est Avg (NTM)']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_collumns = list(data.columns)\n",
    "features = []\n",
    "counter = 14\n",
    "\n",
    "for i in all_collumns[0:15]:\n",
    "    features.append(i.split(\" \", 1)[1])\n",
    "\n",
    "features[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial format of our data, however, is not well-suited for machine learning analysis as it lists companies’ data in a horizontal fashion. Hence, we create 55 separate data frames, each containing data for an individual company. Furthermore, we create some additional features such as stock price standard deviation over 5, 10 and 15 days and percentage of each recommendation type out of total number of recommendations (see below for full feature list). Finally, we create sell, hold and buy labels for 15 days in the future.\n",
    "\n",
    "First 5 rows of an individual dataframe for Apple can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj. Close</th>\n",
       "      <th>P/E (LTM)</th>\n",
       "      <th>EPS - Est High (NTM)</th>\n",
       "      <th>EPS - Est Low (NTM)</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SI (%)</th>\n",
       "      <th>Vol</th>\n",
       "      <th># Buys</th>\n",
       "      <th>...</th>\n",
       "      <th>day_6</th>\n",
       "      <th>day_7</th>\n",
       "      <th>day_8</th>\n",
       "      <th>day_9</th>\n",
       "      <th>day_10</th>\n",
       "      <th>day_11</th>\n",
       "      <th>day_12</th>\n",
       "      <th>day_13</th>\n",
       "      <th>day_14</th>\n",
       "      <th>day_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/9/2003</td>\n",
       "      <td>27.905</td>\n",
       "      <td>17.580627</td>\n",
       "      <td>35.994469</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.900</td>\n",
       "      <td>62048642.0</td>\n",
       "      <td>0.953692</td>\n",
       "      <td>0.307837</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/10/2003</td>\n",
       "      <td>27.960</td>\n",
       "      <td>17.615278</td>\n",
       "      <td>36.065413</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.900</td>\n",
       "      <td>67985474.0</td>\n",
       "      <td>0.953692</td>\n",
       "      <td>0.306803</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/13/2003</td>\n",
       "      <td>28.195</td>\n",
       "      <td>17.763332</td>\n",
       "      <td>36.368538</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.900</td>\n",
       "      <td>61040600.0</td>\n",
       "      <td>0.953692</td>\n",
       "      <td>0.305531</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/14/2003</td>\n",
       "      <td>28.485</td>\n",
       "      <td>17.946037</td>\n",
       "      <td>36.742607</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.915</td>\n",
       "      <td>54441838.0</td>\n",
       "      <td>0.953692</td>\n",
       "      <td>0.304476</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/15/2003</td>\n",
       "      <td>28.135</td>\n",
       "      <td>17.725531</td>\n",
       "      <td>36.291144</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.915</td>\n",
       "      <td>59999384.0</td>\n",
       "      <td>0.953692</td>\n",
       "      <td>0.284455</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date   Close  Adj. Close  P/E (LTM)  EPS - Est High (NTM)  \\\n",
       "0   1/9/2003  27.905   17.580627  35.994469                  1.08   \n",
       "1  1/10/2003  27.960   17.615278  36.065413                  1.08   \n",
       "2  1/13/2003  28.195   17.763332  36.368538                  1.08   \n",
       "3  1/14/2003  28.485   17.946037  36.742607                  1.08   \n",
       "4  1/15/2003  28.135   17.725531  36.291144                  1.08   \n",
       "\n",
       "   EPS - Est Low (NTM)      Volume    SI (%)       Vol  # Buys  ...  day_6  \\\n",
       "0                0.900  62048642.0  0.953692  0.307837    14.0  ...    0.0   \n",
       "1                0.900  67985474.0  0.953692  0.306803    13.0  ...    0.0   \n",
       "2                0.900  61040600.0  0.953692  0.305531    13.0  ...    0.0   \n",
       "3                0.915  54441838.0  0.953692  0.304476    13.0  ...    0.0   \n",
       "4                0.915  59999384.0  0.953692  0.284455    13.0  ...    0.0   \n",
       "\n",
       "   day_7  day_8  day_9  day_10  day_11  day_12  day_13  day_14  day_15  \n",
       "0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "1    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "2    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "3    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "4    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Separating the data into dataframes for each company, dealing with N/A's and creating new features;\n",
    "#Having perfomred the above manipulations, we merge data into one DataFrame\n",
    "\n",
    "dfs = []\n",
    "indices = indices = list(range(0,784,14))\n",
    "counter = 1\n",
    "\n",
    "for i in indices[0:-1]:\n",
    "    df = pd.DataFrame(np.nan, np.arange(4259), columns=features)\n",
    "    #Setting the first column equal to date\n",
    "    df.iloc[:,0]=data.iloc[:,0]\n",
    "\n",
    "    if i != 756:\n",
    "        start = i+1\n",
    "        stop = indices[counter]+1\n",
    "        df.iloc[:,1:15] = data.iloc[:,start:stop]\n",
    "    \n",
    "    else:\n",
    "        start = i\n",
    "        stop = 756 + 15\n",
    "        df.iloc[:,1:15] = data.iloc[:,start:stop]\n",
    "\n",
    "    # Dealing with N/A's\n",
    "    \n",
    "   \n",
    "\n",
    "    # df['Total Rec'] = df['# Buys'] + df['# Sell'] + df['# Hold']\n",
    "\n",
    "    #Total number of reccomentaions and % of each recommnedtion type (buy, sell, hold)\n",
    "    df.insert(12,'Total Rec', 0)\n",
    "    df.iloc[:,12] = df['# Buys'] + df['# Sell'] + df['# Hold']\n",
    "    df.insert(13,'% Buy', 0)\n",
    "    df.insert(14,'% Sell', 0)\n",
    "    df.insert(15,'% Hold', 0)\n",
    "\n",
    "    pc_change = 0.01\n",
    "\n",
    "    for n in range(0,len(df.index)-15):\n",
    "\n",
    "        total_recs = df.iloc[n]['Total Rec']\n",
    "\n",
    "        if (total_recs != 0):\n",
    "            df.at[n, '% Buy'] = df.iloc[n]['# Buys']/total_recs\n",
    "\n",
    "            df.at[n, '% Sell'] = df.iloc[n]['# Sell']/total_recs\n",
    "\n",
    "            df.at[n, '% Hold'] = df.iloc[n]['# Hold']/total_recs\n",
    "        \n",
    "        else:\n",
    "            df.at[n, '% Buy'] = 0\n",
    "\n",
    "            df.at[n, '% Sell'] = 0\n",
    "\n",
    "            df.at[n, '% Hold'] = 0\n",
    "    \n",
    "       \n",
    "        price_0 = df.iloc[n]['Close']      \n",
    "        price_1 = df.iloc[n+1]['Close'] \n",
    "        price_2 = df.iloc[n+2]['Close'] \n",
    "        price_3 = df.iloc[n+3]['Close']\n",
    "        price_4 = df.iloc[n+4]['Close'] \n",
    "        price_5 = df.iloc[n+5]['Close']\n",
    "        price_6 = df.iloc[n+6]['Close'] \n",
    "        price_7 = df.iloc[n+7]['Close']\n",
    "        price_8 = df.iloc[n+8]['Close'] \n",
    "        price_9 = df.iloc[n+9]['Close']\n",
    "        price_10 = df.iloc[n+10]['Close']\n",
    "        price_11 = df.iloc[n+11]['Close']\n",
    "        price_12 = df.iloc[n+12]['Close']\n",
    "        price_13 = df.iloc[n+13]['Close']\n",
    "        price_14 = df.iloc[n+14]['Close']\n",
    "        price_15 = df.iloc[n+15]['Close']\n",
    "\n",
    "        price_days = [price_0, price_1, price_2, price_3, price_4, price_5, price_6, price_7, price_8,\n",
    "        price_9, price_10, price_11, price_12, price_13, price_14, price_15]\n",
    "\n",
    "        df.at[n+5,'std_5days'] = np.std(price_days[0:5], ddof=1) \n",
    "        df.at[n+5,'std_10days'] = np.std(price_days[0:10], ddof=1)\n",
    "        df.at[n+15,'std_15days'] = np.std(price_days[0:15], ddof=1) \n",
    "\n",
    "\n",
    "        df.at[n+5, '% change_5d'] = (price_4-price_0)/price_0\n",
    "        df.at[n+10, '% change_5d'] = (price_9-price_0)/price_0\n",
    "        df.at[n+15, '% change_5d'] = (price_14-price_0)/price_0\n",
    "\n",
    "\n",
    "        for j in range(0, 16):\n",
    "\n",
    "            if j==0:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                # buy=2 sell=0 hold=1\n",
    "                if((price_days[j]-price_0) >= pc_change * price_0):\n",
    "                    df.at[n,'day_' + str(j)] = 2\n",
    "\n",
    "                elif((price_days[j]-price_0) <= -(pc_change * price_0)):\n",
    "                    df.at[n,'day_' + str(j)] = 0\n",
    "\n",
    "                else:\n",
    "                    df.at[n,'day_' + str(j)] = 1\n",
    "\n",
    "    #Truncating the data for younger companies with N/A's only at the start of the 2000's  \n",
    "    df.iloc[:,1:] = df.iloc[:,1:].astype(float).interpolate()\n",
    "    df.dropna(thresh=35, inplace=True)\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # df.dropna(subset=['Close'], inplace=True)\n",
    "\n",
    "    dfs.append(df)\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "dfs[1].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " 'Close',\n",
       " 'Adj. Close',\n",
       " 'P/E (LTM)',\n",
       " 'EPS - Est High (NTM)',\n",
       " 'EPS - Est Low (NTM)',\n",
       " 'Volume',\n",
       " 'SI (%)',\n",
       " 'Vol',\n",
       " '# Buys',\n",
       " '# Sell',\n",
       " '# Hold',\n",
       " 'Total Rec',\n",
       " '% Buy',\n",
       " '% Sell',\n",
       " '% Hold',\n",
       " 'Rating',\n",
       " 'Mkt Cap',\n",
       " 'EPS - Est Avg (NTM)',\n",
       " 'std_5days',\n",
       " 'std_10days',\n",
       " 'std_15days',\n",
       " '% change_5d',\n",
       " 'day_1',\n",
       " 'day_2',\n",
       " 'day_3',\n",
       " 'day_4',\n",
       " 'day_5',\n",
       " 'day_6',\n",
       " 'day_7',\n",
       " 'day_8',\n",
       " 'day_9',\n",
       " 'day_10',\n",
       " 'day_11',\n",
       " 'day_12',\n",
       " 'day_13',\n",
       " 'day_14',\n",
       " 'day_15']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All features + labels\n",
    "all_collumns = list(dfs[1].columns)\n",
    "all_collumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then merge all dataframes to form a new long dataframe, which is much more well-suited for machine learning analysis, with total of 151,424 rows.\n",
    "\n",
    "It is important to note that we are well aware of all caveats associated with working with time-series data, and that we do not run any root tests nor apply any related transformations for a simple reason: we are interested in the predictive capability of our features irrespective of the time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_long = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the models do not pick up on any long-term trends while training we shuffle the data (and reset the index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = data_long.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis and Visualisations\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.294879719639335 0.2762635601577909 0.4288567202028741\n"
     ]
    }
   ],
   "source": [
    "pc_sell = vis1.loc[:,'#Sell'].sum()/vis1.loc[:,'#Total'].sum()\n",
    "pc_hold = vis1.loc[:,'#Hold'].sum()/vis1.loc[:,'#Total'].sum()\n",
    "pc_buy = vis1.loc[:,'#Buy'].sum()/vis1.loc[:,'#Total'].sum()\n",
    "print(pc_sell, pc_hold, pc_buy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we would like to see whether an investors strategy (and the position he favors today) might change depending on the time horizon at the end of which he expects to collect his returns.  To do this we plot what would his strategy be if he knew the future price one day at a time over 15 days in the future. It appears that his strategy would change drastically depending on the time horizon – at the start of the horizon there is a 60% chance that his preferred strategy would be to hold a stock, and only around 20% that he should sell or buy it. Looking at the stock’s price on the 15th day, however, today he would prefer to buy more with a 52% chance, sell with a 32% chance and hold with a 16% chance. We hence, should expect our models to give different prediction given the day label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1b991033cd0>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b990e88250>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1b991033cd0>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df_new\n",
    "\n",
    "#Separating df into features and labels  \n",
    "features = df.iloc[:,1:23]\n",
    "labels = df.iloc[:,23:]\n",
    "\n",
    "vis1_col = ['#Sell','#Hold', '#Buy']\n",
    "days = list(labels.columns)\n",
    "\n",
    "vis1 = pd.DataFrame(index=days, columns = vis1_col)\n",
    "\n",
    "for i in days:\n",
    "    vis1.loc[i, '#Sell'] = (df.loc[:,i] == 0).sum()\n",
    "    vis1.loc[i, '#Hold'] = (df.loc[:,i] == 1).sum()\n",
    "    vis1.loc[i, '#Buy'] = (df.loc[:,i] == 2).sum()\n",
    "    vis1['#Total'] = vis1['#Sell'] + vis1['#Hold'] + vis1['#Buy']\n",
    "    # vis1.loc[:,'#Total'] = vis1.loc[:,'#Sell'] + vis1.loc[:,'#Hold'] + vis1.loc[:, '#Buy']\n",
    "    vis1.loc[:, '% Sell'] = (vis1.loc[:, '#Sell']/vis1.loc[:,'#Total'])*100\n",
    "    vis1.loc[:, '% Hold'] = (vis1.loc[:, '#Hold']/vis1.loc[:,'#Total'])*100\n",
    "    vis1.loc[:, '% Buy'] = (vis1.loc[:, '#Buy']/vis1.loc[:,'#Total'])*100\n",
    "\n",
    "vis1.insert(0,'Day_n', list(range(1,16)))\n",
    "vis1.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "vis1 = vis1.astype(float).round()\n",
    "vis11 = vis1.iloc[:, 5:]\n",
    "vis11.rename(columns={'% Sell':'Sell','% Hold':'Hold', '% Buy':'Buy'},\n",
    "inplace=True)\n",
    "vis11.insert(0,'Day_n', list(range(1,16)))\n",
    "vis11 = vis11.iloc[::2,:]\n",
    "vis11 = pd.melt(vis11, id_vars=\"Day_n\", var_name=\"Current favourable postion\", value_name=\"%\")\n",
    "# vis11\n",
    "sns.set_theme()\n",
    "\n",
    "vis1_sns = sns.factorplot(data=vis11, x='Day_n', y='%', \n",
    "hue='Current favourable postion', kind='bar', legend=False)\n",
    "\n",
    "vis1_sns.set(xlabel='Days in the future', ylabel='%')\n",
    "plt.legend(loc='upper center', title='Current Favourable Postion')\n",
    "\n",
    "vis1_sns.set(title='Percentage of Labels Values for Nth Day')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might argue that this finding suggest that we might have to rebalance our classes. We counter this critique by saying two thigs. First, we intend to use F1 Weighted as the scoring techniques, which accounts for possible problems associated with imbalanced classes. Second, we do not wish to lose valuable information which potentially lies in the imbalanced classes. This is particularly important given that the smooth pattern of the graph above suggests that this imbalance is not random. Lastly, our test data will have the same distribution as above, meaning that it should not be a problem within context of this paper.\n",
    "\n",
    "Now we turn our attention to a diffrenet visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Scatterplot of Position Types Throughout Days')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x1b9c9157250>,\n",
       "  <matplotlib.axis.XTick at 0x1b9c91574c0>,\n",
       "  <matplotlib.axis.XTick at 0x1b9c9156650>],\n",
       " [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Text(-2.0, 0, 'Sell'),\n",
       " Text(0.0, 0, 'Hold'),\n",
       " Text(2.0, 0, 'Buy'),\n",
       " Text(4.0, 0, '')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, ''), Text(0, 0.5, 'P/E Ratio')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Day 1')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Text(-2.0, 0, 'Sell'),\n",
       " Text(0.0, 0, 'Hold'),\n",
       " Text(2.0, 0, 'Buy'),\n",
       " Text(4.0, 0, '')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Position_Type'), Text(0, 0.5, ' ')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Day 8')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, 'Sell'), Text(1, 0, 'Hold'), Text(2, 0, 'Buy')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, ''), Text(0, 0.5, ' ')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Day 15')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df_new\n",
    "\n",
    "d = {'Position_Type':['Sell', 'Hold', 'Buy']}\n",
    "vis2 = pd.DataFrame(d)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharey=True)\n",
    "fig.suptitle('Scatterplot of Position Types Throughout Days')\n",
    "# plt.xticks(np.arange(0, 3, 1), labels=['a','b','c'])\n",
    "\n",
    "plt.xticks(np.arange(0, 3, 1))\n",
    "\n",
    "vis21_sns = sns.regplot(ax=ax1, data=df,  x=\"day_1\", y=\"P/E (LTM)\", fit_reg=False)\n",
    "\n",
    "\n",
    "vis21_sns.set_xticklabels(vis2['Position_Type'], rotation='horizontal', fontsize=11)\n",
    "vis21_sns.set(xlabel='', ylabel='P/E Ratio')\n",
    "ax1.set_title('Day 1')\n",
    "\n",
    "\n",
    "\n",
    "vis22_sns = sns.regplot(ax=ax2, data=df,  x=\"day_8\", y=\"P/E (LTM)\", fit_reg=False)\n",
    "# plt.xticks(np.arange(0, 5, 1))\n",
    "vis22_sns.set_xticklabels(vis2['Position_Type'], rotation='horizontal', fontsize=11)\n",
    "vis22_sns.set(xlabel='Position_Type', ylabel=' ')\n",
    "ax2.set_title('Day 8')\n",
    "\n",
    "\n",
    "vis23_sns = sns.regplot(ax=ax3, data=df,  x=\"day_15\", y=\"P/E (LTM)\", fit_reg=False)\n",
    "# plt.xticks(np.arange(0, 3, 1))\n",
    "vis23_sns.set_xticklabels(vis2['Position_Type'], rotation='horizontal', fontsize=11)\n",
    "vis23_sns.set(xlabel='', ylabel=' ')\n",
    "ax3.set_title('Day 15')\n",
    "\n",
    "\n",
    "\n",
    "# Move the legend to an empty part of the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Please note that the labels should be in order Sell, Hold, Buy- from right to the left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the original models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section applies some of the original models from Matloob and Khushi (2021) to our data. In particular we apply Random Forest, KNNeighbors, a Multi Layered Perceptron and Logistic Regression Classifiers. We then store the classification scores (on test data) and compare them to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to acuumulate original models scores\n",
    "# (Partly based on the code from Matloob and Khushi (2021))\n",
    "(https://github.com/sjdee/Research-Stock-Prediction)\n",
    "\n",
    "def accumulate_data(report_data,report,accuracy,day_name,model_name,minify_data,f1_scores):\n",
    "  \n",
    "  print(day_name)\n",
    "  print(report)\n",
    "  \n",
    "  if(minify_data == True):\n",
    "\n",
    "    row = {}\n",
    "    row['day'] = day_name.replace(\"day_\", \"\") \n",
    "    row['accuracy'] = accuracy\n",
    "    row['f1_weigthed'] = f1_scores[0]\n",
    "    row['model'] = model_name\n",
    "\n",
    "    # unravel report for the given day       \n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-5]:\n",
    "\n",
    "      row_data = line.split('     ')\n",
    "\n",
    "      # update recall for sell\n",
    "      if(float(row_data[1])==0.0):\n",
    "        row['sell_recall']= float(row_data[3])\n",
    "      # update precison for buy\n",
    "      if(float(row_data[1])==2.0):\n",
    "        row['buy_precison']= float(row_data[2])\n",
    "\n",
    "    report_data.append(row)\n",
    "\n",
    "\n",
    "  else:    \n",
    "    print('This is not to be used')\n",
    "\n",
    "  return report_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function to run a classifier for all 15 labels (days)\n",
    "# (Based on the code from Matloob and Khushi (2021))\n",
    "def run_classifier (pDf, m, params, model_name, minify_data, less_columns, print_data):  \n",
    "\n",
    "  \n",
    "  df = pDf\n",
    "  \n",
    "  df.dropna(inplace=True)\n",
    "  features = df.iloc[:,1:23]\n",
    "  labels = df.iloc[:,23:]\n",
    "    \n",
    "  report_data = []\n",
    "\n",
    "\n",
    "  for i in range(len(labels.columns)):\n",
    "    # specify the feature set, target set, the test size and random_state to select records randomly\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels.iloc[:,i], test_size=0.3,random_state=0) \n",
    "\n",
    "    # Scaling values in the feature set\n",
    "    scaling = MinMaxScaler(feature_range=(0,1)).fit(X_train)\n",
    "    X_train = scaling.transform(X_train)\n",
    "    X_test = scaling.transform(X_test)\n",
    "\n",
    "\n",
    "    # Create a decision tree Classifier\n",
    "    clf = m().set_params(**params)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    f1_scores =[] \n",
    "    f1_scores.insert(2, metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    accumulate_data(report_data,report,accuracy,labels.columns[i], model_name, minify_data,f1_scores)\n",
    "\n",
    "\n",
    "    \n",
    "  return report_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_weigthed</th>\n",
       "      <th>model</th>\n",
       "      <th>sell_recall</th>\n",
       "      <th>buy_precison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.566435</td>\n",
       "      <td>0.531608</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.518469</td>\n",
       "      <td>0.511131</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.547592</td>\n",
       "      <td>0.546958</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.564806</td>\n",
       "      <td>0.565081</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.563089</td>\n",
       "      <td>0.562792</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.589284</td>\n",
       "      <td>0.587561</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.617086</td>\n",
       "      <td>0.612837</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.654134</td>\n",
       "      <td>0.649125</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.670556</td>\n",
       "      <td>0.662363</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.691578</td>\n",
       "      <td>0.682342</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.705336</td>\n",
       "      <td>0.696017</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.729638</td>\n",
       "      <td>0.719227</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.735251</td>\n",
       "      <td>0.723136</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.757815</td>\n",
       "      <td>0.746166</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.763802</td>\n",
       "      <td>0.752039</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  accuracy  f1_weigthed          model  sell_recall  buy_precison\n",
       "0    1  0.566435     0.531608  Decision Tree         0.21          0.31\n",
       "1    2  0.518469     0.511131  Decision Tree         0.38          0.48\n",
       "2    3  0.547592     0.546958  Decision Tree         0.49          0.57\n",
       "3    4  0.564806     0.565081  Decision Tree         0.54          0.61\n",
       "4    5  0.563089     0.562792  Decision Tree         0.56          0.63\n",
       "5    6  0.589284     0.587561  Decision Tree         0.61          0.67\n",
       "6    7  0.617086     0.612837  Decision Tree         0.65          0.69\n",
       "7    8  0.654134     0.649125  Decision Tree         0.69          0.73\n",
       "8    9  0.670556     0.662363  Decision Tree         0.71          0.74\n",
       "9   10  0.691578     0.682342  Decision Tree         0.74          0.75\n",
       "10  11  0.705336     0.696017  Decision Tree         0.74          0.77\n",
       "11  12  0.729638     0.719227  Decision Tree         0.78          0.79\n",
       "12  13  0.735251     0.723136  Decision Tree         0.78          0.79\n",
       "13  14  0.757815     0.746166  Decision Tree         0.80          0.82\n",
       "14  15  0.763802     0.752039  Decision Tree         0.81          0.82"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      0.21      0.24      8224\n",
      "         1.0       0.66      0.81      0.73     27560\n",
      "         2.0       0.31      0.18      0.22      9644\n",
      "\n",
      "    accuracy                           0.57     45428\n",
      "   macro avg       0.42      0.40      0.40     45428\n",
      "weighted avg       0.52      0.57      0.53     45428\n",
      "\n",
      "day_2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.38      0.40     10838\n",
      "         1.0       0.57      0.67      0.62     21014\n",
      "         2.0       0.48      0.40      0.44     13576\n",
      "\n",
      "    accuracy                           0.52     45428\n",
      "   macro avg       0.49      0.48      0.48     45428\n",
      "weighted avg       0.51      0.52      0.51     45428\n",
      "\n",
      "day_3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.49      0.50     12116\n",
      "         1.0       0.55      0.60      0.58     17512\n",
      "         2.0       0.57      0.53      0.55     15800\n",
      "\n",
      "    accuracy                           0.55     45428\n",
      "   macro avg       0.54      0.54      0.54     45428\n",
      "weighted avg       0.55      0.55      0.55     45428\n",
      "\n",
      "day_4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.54      0.54     13032\n",
      "         1.0       0.53      0.54      0.53     15011\n",
      "         2.0       0.61      0.60      0.61     17385\n",
      "\n",
      "    accuracy                           0.56     45428\n",
      "   macro avg       0.56      0.56      0.56     45428\n",
      "weighted avg       0.57      0.56      0.57     45428\n",
      "\n",
      "day_5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.56      0.56     13315\n",
      "         1.0       0.48      0.47      0.47     13586\n",
      "         2.0       0.63      0.64      0.63     18527\n",
      "\n",
      "    accuracy                           0.56     45428\n",
      "   macro avg       0.55      0.55      0.55     45428\n",
      "weighted avg       0.56      0.56      0.56     45428\n",
      "\n",
      "day_6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.61      0.60     13618\n",
      "         1.0       0.46      0.43      0.44     12280\n",
      "         2.0       0.67      0.68      0.67     19530\n",
      "\n",
      "    accuracy                           0.59     45428\n",
      "   macro avg       0.57      0.57      0.57     45428\n",
      "weighted avg       0.59      0.59      0.59     45428\n",
      "\n",
      "day_7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.65      0.63     13845\n",
      "         1.0       0.45      0.39      0.42     11424\n",
      "         2.0       0.69      0.72      0.71     20159\n",
      "\n",
      "    accuracy                           0.62     45428\n",
      "   macro avg       0.59      0.59      0.59     45428\n",
      "weighted avg       0.61      0.62      0.61     45428\n",
      "\n",
      "day_8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.69      0.68     14153\n",
      "         1.0       0.46      0.39      0.42     10523\n",
      "         2.0       0.73      0.76      0.74     20752\n",
      "\n",
      "    accuracy                           0.65     45428\n",
      "   macro avg       0.62      0.61      0.62     45428\n",
      "weighted avg       0.65      0.65      0.65     45428\n",
      "\n",
      "day_9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.71      0.70     14146\n",
      "         1.0       0.46      0.36      0.40     10071\n",
      "         2.0       0.74      0.79      0.76     21211\n",
      "\n",
      "    accuracy                           0.67     45428\n",
      "   macro avg       0.63      0.62      0.62     45428\n",
      "weighted avg       0.66      0.67      0.66     45428\n",
      "\n",
      "day_10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.74      0.72     14302\n",
      "         1.0       0.47      0.35      0.40      9479\n",
      "         2.0       0.75      0.81      0.78     21647\n",
      "\n",
      "    accuracy                           0.69     45428\n",
      "   macro avg       0.64      0.63      0.63     45428\n",
      "weighted avg       0.68      0.69      0.68     45428\n",
      "\n",
      "day_11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.74      0.73     14377\n",
      "         1.0       0.46      0.35      0.39      8973\n",
      "         2.0       0.77      0.83      0.80     22078\n",
      "\n",
      "    accuracy                           0.71     45428\n",
      "   macro avg       0.65      0.64      0.64     45428\n",
      "weighted avg       0.69      0.71      0.70     45428\n",
      "\n",
      "day_12\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.78      0.76     14463\n",
      "         1.0       0.47      0.34      0.40      8526\n",
      "         2.0       0.79      0.85      0.82     22439\n",
      "\n",
      "    accuracy                           0.73     45428\n",
      "   macro avg       0.67      0.66      0.66     45428\n",
      "weighted avg       0.71      0.73      0.72     45428\n",
      "\n",
      "day_13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.78      0.76     14435\n",
      "         1.0       0.46      0.32      0.37      8128\n",
      "         2.0       0.79      0.86      0.82     22865\n",
      "\n",
      "    accuracy                           0.74     45428\n",
      "   macro avg       0.67      0.65      0.65     45428\n",
      "weighted avg       0.72      0.74      0.72     45428\n",
      "\n",
      "day_14\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.80      0.78     14480\n",
      "         1.0       0.49      0.33      0.40      7775\n",
      "         2.0       0.82      0.87      0.84     23173\n",
      "\n",
      "    accuracy                           0.76     45428\n",
      "   macro avg       0.69      0.67      0.67     45428\n",
      "weighted avg       0.74      0.76      0.75     45428\n",
      "\n",
      "day_15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.81      0.79     14487\n",
      "         1.0       0.48      0.32      0.39      7488\n",
      "         2.0       0.82      0.88      0.85     23453\n",
      "\n",
      "    accuracy                           0.76     45428\n",
      "   macro avg       0.69      0.67      0.67     45428\n",
      "weighted avg       0.75      0.76      0.75     45428\n",
      "\n",
      "\n",
      " Completed fitting RF \n",
      "\n",
      "day_1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.26      0.24      0.25      8224\n",
      "         1.0       0.66      0.81      0.73     27560\n",
      "         2.0       0.28      0.11      0.16      9644\n",
      "\n",
      "    accuracy                           0.56     45428\n",
      "   macro avg       0.40      0.39      0.38     45428\n",
      "weighted avg       0.50      0.56      0.52     45428\n",
      "\n",
      "day_2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.34      0.38      0.36     10838\n",
      "         1.0       0.55      0.65      0.59     21014\n",
      "         2.0       0.43      0.27      0.33     13576\n",
      "\n",
      "    accuracy                           0.47     45428\n",
      "   macro avg       0.44      0.43      0.43     45428\n",
      "weighted avg       0.46      0.47      0.46     45428\n",
      "\n",
      "day_3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.48      0.44     12116\n",
      "         1.0       0.49      0.54      0.52     17512\n",
      "         2.0       0.51      0.38      0.44     15800\n",
      "\n",
      "    accuracy                           0.47     45428\n",
      "   macro avg       0.47      0.47      0.46     45428\n",
      "weighted avg       0.48      0.47      0.47     45428\n",
      "\n",
      "day_4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.53      0.49     13032\n",
      "         1.0       0.46      0.47      0.47     15011\n",
      "         2.0       0.55      0.46      0.51     17385\n",
      "\n",
      "    accuracy                           0.49     45428\n",
      "   macro avg       0.49      0.49      0.49     45428\n",
      "weighted avg       0.49      0.49      0.49     45428\n",
      "\n",
      "day_5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.57      0.52     13315\n",
      "         1.0       0.44      0.42      0.43     13586\n",
      "         2.0       0.60      0.53      0.56     18527\n",
      "\n",
      "    accuracy                           0.51     45428\n",
      "   macro avg       0.51      0.51      0.51     45428\n",
      "weighted avg       0.52      0.51      0.51     45428\n",
      "\n",
      "day_6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.60      0.55     13618\n",
      "         1.0       0.42      0.38      0.40     12280\n",
      "         2.0       0.63      0.58      0.61     19530\n",
      "\n",
      "    accuracy                           0.53     45428\n",
      "   macro avg       0.52      0.52      0.52     45428\n",
      "weighted avg       0.54      0.53      0.53     45428\n",
      "\n",
      "day_7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.63      0.57     13845\n",
      "         1.0       0.41      0.35      0.38     11424\n",
      "         2.0       0.65      0.62      0.64     20159\n",
      "\n",
      "    accuracy                           0.55     45428\n",
      "   macro avg       0.53      0.53      0.53     45428\n",
      "weighted avg       0.55      0.55      0.55     45428\n",
      "\n",
      "day_8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.63      0.59     14153\n",
      "         1.0       0.40      0.34      0.37     10523\n",
      "         2.0       0.67      0.65      0.66     20752\n",
      "\n",
      "    accuracy                           0.57     45428\n",
      "   macro avg       0.54      0.54      0.54     45428\n",
      "weighted avg       0.57      0.57      0.57     45428\n",
      "\n",
      "day_9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.67      0.61     14146\n",
      "         1.0       0.41      0.32      0.36     10071\n",
      "         2.0       0.69      0.68      0.68     21211\n",
      "\n",
      "    accuracy                           0.59     45428\n",
      "   macro avg       0.55      0.56      0.55     45428\n",
      "weighted avg       0.59      0.59      0.59     45428\n",
      "\n",
      "day_10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.68      0.62     14302\n",
      "         1.0       0.40      0.31      0.35      9479\n",
      "         2.0       0.71      0.70      0.70     21647\n",
      "\n",
      "    accuracy                           0.61     45428\n",
      "   macro avg       0.56      0.56      0.56     45428\n",
      "weighted avg       0.60      0.61      0.60     45428\n",
      "\n",
      "day_11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.69      0.64     14377\n",
      "         1.0       0.39      0.30      0.34      8973\n",
      "         2.0       0.72      0.71      0.72     22078\n",
      "\n",
      "    accuracy                           0.63     45428\n",
      "   macro avg       0.57      0.57      0.57     45428\n",
      "weighted avg       0.62      0.63      0.62     45428\n",
      "\n",
      "day_12\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.70      0.65     14463\n",
      "         1.0       0.38      0.29      0.33      8526\n",
      "         2.0       0.73      0.73      0.73     22439\n",
      "\n",
      "    accuracy                           0.64     45428\n",
      "   macro avg       0.57      0.57      0.57     45428\n",
      "weighted avg       0.63      0.64      0.63     45428\n",
      "\n",
      "day_13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.71      0.66     14435\n",
      "         1.0       0.38      0.27      0.32      8128\n",
      "         2.0       0.75      0.75      0.75     22865\n",
      "\n",
      "    accuracy                           0.65     45428\n",
      "   macro avg       0.58      0.58      0.57     45428\n",
      "weighted avg       0.64      0.65      0.64     45428\n",
      "\n",
      "day_14\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.72      0.67     14480\n",
      "         1.0       0.39      0.27      0.32      7775\n",
      "         2.0       0.75      0.76      0.76     23173\n",
      "\n",
      "    accuracy                           0.66     45428\n",
      "   macro avg       0.59      0.58      0.58     45428\n",
      "weighted avg       0.65      0.66      0.65     45428\n",
      "\n",
      "day_15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.73      0.68     14487\n",
      "         1.0       0.38      0.26      0.31      7488\n",
      "         2.0       0.76      0.77      0.77     23453\n",
      "\n",
      "    accuracy                           0.67     45428\n",
      "   macro avg       0.59      0.59      0.58     45428\n",
      "weighted avg       0.66      0.67      0.66     45428\n",
      "\n",
      "\n",
      " Completed fitting KNN \n",
      "\n",
      "day_1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      8224\n",
      "         1.0       0.63      0.96      0.76     27560\n",
      "         2.0       0.36      0.11      0.17      9644\n",
      "\n",
      "    accuracy                           0.61     45428\n",
      "   macro avg       0.33      0.36      0.31     45428\n",
      "weighted avg       0.46      0.61      0.50     45428\n",
      "\n",
      "day_2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     10838\n",
      "         1.0       0.57      0.66      0.61     21014\n",
      "         2.0       0.37      0.56      0.44     13576\n",
      "\n",
      "    accuracy                           0.47     45428\n",
      "   macro avg       0.31      0.41      0.35     45428\n",
      "weighted avg       0.37      0.47      0.42     45428\n",
      "\n",
      "day_3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     12116\n",
      "         1.0       0.51      0.58      0.55     17512\n",
      "         2.0       0.40      0.65      0.50     15800\n",
      "\n",
      "    accuracy                           0.45     45428\n",
      "   macro avg       0.30      0.41      0.35     45428\n",
      "weighted avg       0.34      0.45      0.38     45428\n",
      "\n",
      "day_4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     13032\n",
      "         1.0       0.46      0.51      0.48     15011\n",
      "         2.0       0.43      0.70      0.53     17385\n",
      "\n",
      "    accuracy                           0.44     45428\n",
      "   macro avg       0.29      0.41      0.34     45428\n",
      "weighted avg       0.31      0.44      0.36     45428\n",
      "\n",
      "day_5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     13315\n",
      "         1.0       0.42      0.46      0.44     13586\n",
      "         2.0       0.44      0.72      0.55     18527\n",
      "\n",
      "    accuracy                           0.43     45428\n",
      "   macro avg       0.29      0.39      0.33     45428\n",
      "weighted avg       0.30      0.43      0.35     45428\n",
      "\n",
      "day_6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     13618\n",
      "         1.0       0.42      0.21      0.28     12280\n",
      "         2.0       0.45      0.90      0.60     19530\n",
      "\n",
      "    accuracy                           0.44     45428\n",
      "   macro avg       0.29      0.37      0.29     45428\n",
      "weighted avg       0.31      0.44      0.33     45428\n",
      "\n",
      "day_7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     13845\n",
      "         1.0       0.41      0.13      0.20     11424\n",
      "         2.0       0.45      0.94      0.61     20159\n",
      "\n",
      "    accuracy                           0.45     45428\n",
      "   macro avg       0.29      0.36      0.27     45428\n",
      "weighted avg       0.30      0.45      0.32     45428\n",
      "\n",
      "day_8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14153\n",
      "         1.0       0.00      0.00      0.00     10523\n",
      "         2.0       0.46      1.00      0.63     20752\n",
      "\n",
      "    accuracy                           0.46     45428\n",
      "   macro avg       0.15      0.33      0.21     45428\n",
      "weighted avg       0.21      0.46      0.29     45428\n",
      "\n",
      "day_9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14146\n",
      "         1.0       0.00      0.00      0.00     10071\n",
      "         2.0       0.47      1.00      0.64     21211\n",
      "\n",
      "    accuracy                           0.47     45428\n",
      "   macro avg       0.16      0.33      0.21     45428\n",
      "weighted avg       0.22      0.47      0.30     45428\n",
      "\n",
      "day_10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14302\n",
      "         1.0       0.00      0.00      0.00      9479\n",
      "         2.0       0.48      1.00      0.65     21647\n",
      "\n",
      "    accuracy                           0.48     45428\n",
      "   macro avg       0.16      0.33      0.22     45428\n",
      "weighted avg       0.23      0.48      0.31     45428\n",
      "\n",
      "day_11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.02      0.05     14377\n",
      "         1.0       0.00      0.00      0.00      8973\n",
      "         2.0       0.49      0.98      0.65     22078\n",
      "\n",
      "    accuracy                           0.48     45428\n",
      "   macro avg       0.29      0.34      0.23     45428\n",
      "weighted avg       0.36      0.48      0.33     45428\n",
      "\n",
      "day_12\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14463\n",
      "         1.0       0.00      0.00      0.00      8526\n",
      "         2.0       0.49      1.00      0.66     22439\n",
      "\n",
      "    accuracy                           0.49     45428\n",
      "   macro avg       0.16      0.33      0.22     45428\n",
      "weighted avg       0.24      0.49      0.33     45428\n",
      "\n",
      "day_13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14435\n",
      "         1.0       0.00      0.00      0.00      8128\n",
      "         2.0       0.50      1.00      0.67     22865\n",
      "\n",
      "    accuracy                           0.50     45428\n",
      "   macro avg       0.17      0.33      0.22     45428\n",
      "weighted avg       0.25      0.50      0.34     45428\n",
      "\n",
      "day_14\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14480\n",
      "         1.0       0.00      0.00      0.00      7775\n",
      "         2.0       0.51      1.00      0.68     23173\n",
      "\n",
      "    accuracy                           0.51     45428\n",
      "   macro avg       0.17      0.33      0.23     45428\n",
      "weighted avg       0.26      0.51      0.34     45428\n",
      "\n",
      "day_15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14487\n",
      "         1.0       0.00      0.00      0.00      7488\n",
      "         2.0       0.52      1.00      0.68     23453\n",
      "\n",
      "    accuracy                           0.52     45428\n",
      "   macro avg       0.17      0.33      0.23     45428\n",
      "weighted avg       0.27      0.52      0.35     45428\n",
      "\n",
      "\n",
      " Completed fitting DTree \n",
      "\n",
      "day_1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      8224\n",
      "         1.0       0.61      1.00      0.76     27560\n",
      "         2.0       0.00      0.00      0.00      9644\n",
      "\n",
      "    accuracy                           0.61     45428\n",
      "   macro avg       0.20      0.33      0.25     45428\n",
      "weighted avg       0.37      0.61      0.46     45428\n",
      "\n",
      "day_2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     10838\n",
      "         1.0       0.59      0.66      0.62     21014\n",
      "         2.0       0.38      0.62      0.47     13576\n",
      "\n",
      "    accuracy                           0.49     45428\n",
      "   macro avg       0.32      0.43      0.37     45428\n",
      "weighted avg       0.39      0.49      0.43     45428\n",
      "\n",
      "day_3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     12116\n",
      "         1.0       0.56      0.61      0.59     17512\n",
      "         2.0       0.43      0.71      0.53     15800\n",
      "\n",
      "    accuracy                           0.48     45428\n",
      "   macro avg       0.33      0.44      0.37     45428\n",
      "weighted avg       0.36      0.48      0.41     45428\n",
      "\n",
      "day_4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     13032\n",
      "         1.0       0.00      0.00      0.00     15011\n",
      "         2.0       0.38      1.00      0.55     17385\n",
      "\n",
      "    accuracy                           0.38     45428\n",
      "   macro avg       0.13      0.33      0.18     45428\n",
      "weighted avg       0.15      0.38      0.21     45428\n",
      "\n",
      "day_5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     13315\n",
      "         1.0       0.00      0.00      0.00     13586\n",
      "         2.0       0.41      1.00      0.58     18527\n",
      "\n",
      "    accuracy                           0.41     45428\n",
      "   macro avg       0.14      0.33      0.19     45428\n",
      "weighted avg       0.17      0.41      0.24     45428\n",
      "\n",
      "day_6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     13618\n",
      "         1.0       0.00      0.00      0.00     12280\n",
      "         2.0       0.43      1.00      0.60     19530\n",
      "\n",
      "    accuracy                           0.43     45428\n",
      "   macro avg       0.14      0.33      0.20     45428\n",
      "weighted avg       0.18      0.43      0.26     45428\n",
      "\n",
      "day_7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     13845\n",
      "         1.0       0.00      0.00      0.00     11424\n",
      "         2.0       0.44      1.00      0.61     20159\n",
      "\n",
      "    accuracy                           0.44     45428\n",
      "   macro avg       0.15      0.33      0.20     45428\n",
      "weighted avg       0.20      0.44      0.27     45428\n",
      "\n",
      "day_8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14153\n",
      "         1.0       0.00      0.00      0.00     10523\n",
      "         2.0       0.46      1.00      0.63     20752\n",
      "\n",
      "    accuracy                           0.46     45428\n",
      "   macro avg       0.15      0.33      0.21     45428\n",
      "weighted avg       0.21      0.46      0.29     45428\n",
      "\n",
      "day_9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14146\n",
      "         1.0       0.00      0.00      0.00     10071\n",
      "         2.0       0.47      1.00      0.64     21211\n",
      "\n",
      "    accuracy                           0.47     45428\n",
      "   macro avg       0.16      0.33      0.21     45428\n",
      "weighted avg       0.22      0.47      0.30     45428\n",
      "\n",
      "day_10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14302\n",
      "         1.0       0.00      0.00      0.00      9479\n",
      "         2.0       0.48      1.00      0.65     21647\n",
      "\n",
      "    accuracy                           0.48     45428\n",
      "   macro avg       0.16      0.33      0.22     45428\n",
      "weighted avg       0.23      0.48      0.31     45428\n",
      "\n",
      "day_11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14377\n",
      "         1.0       0.00      0.00      0.00      8973\n",
      "         2.0       0.49      1.00      0.65     22078\n",
      "\n",
      "    accuracy                           0.49     45428\n",
      "   macro avg       0.16      0.33      0.22     45428\n",
      "weighted avg       0.24      0.49      0.32     45428\n",
      "\n",
      "day_12\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14463\n",
      "         1.0       0.00      0.00      0.00      8526\n",
      "         2.0       0.49      1.00      0.66     22439\n",
      "\n",
      "    accuracy                           0.49     45428\n",
      "   macro avg       0.16      0.33      0.22     45428\n",
      "weighted avg       0.24      0.49      0.33     45428\n",
      "\n",
      "day_13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14435\n",
      "         1.0       0.00      0.00      0.00      8128\n",
      "         2.0       0.50      1.00      0.67     22865\n",
      "\n",
      "    accuracy                           0.50     45428\n",
      "   macro avg       0.17      0.33      0.22     45428\n",
      "weighted avg       0.25      0.50      0.34     45428\n",
      "\n",
      "day_14\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14480\n",
      "         1.0       0.00      0.00      0.00      7775\n",
      "         2.0       0.51      1.00      0.68     23173\n",
      "\n",
      "    accuracy                           0.51     45428\n",
      "   macro avg       0.17      0.33      0.23     45428\n",
      "weighted avg       0.26      0.51      0.34     45428\n",
      "\n",
      "day_15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     14487\n",
      "         1.0       0.00      0.00      0.00      7488\n",
      "         2.0       0.52      1.00      0.68     23453\n",
      "\n",
      "    accuracy                           0.52     45428\n",
      "   macro avg       0.17      0.33      0.23     45428\n",
      "weighted avg       0.27      0.52      0.35     45428\n",
      "\n",
      "\n",
      " Completed fitting MLP \n",
      "\n",
      "day_1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.30      0.01      0.02      8224\n",
      "         1.0       0.62      0.98      0.76     27560\n",
      "         2.0       0.35      0.05      0.09      9644\n",
      "\n",
      "    accuracy                           0.61     45428\n",
      "   macro avg       0.42      0.35      0.29     45428\n",
      "weighted avg       0.50      0.61      0.48     45428\n",
      "\n",
      "day_2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.01      0.02     10838\n",
      "         1.0       0.50      0.93      0.65     21014\n",
      "         2.0       0.41      0.18      0.25     13576\n",
      "\n",
      "    accuracy                           0.48     45428\n",
      "   macro avg       0.42      0.37      0.31     45428\n",
      "weighted avg       0.44      0.48      0.38     45428\n",
      "\n",
      "day_3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.01      0.02     12116\n",
      "         1.0       0.46      0.81      0.59     17512\n",
      "         2.0       0.44      0.40      0.42     15800\n",
      "\n",
      "    accuracy                           0.45     45428\n",
      "   macro avg       0.45      0.41      0.34     45428\n",
      "weighted avg       0.45      0.45      0.38     45428\n",
      "\n",
      "day_4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.01      0.01     13032\n",
      "         1.0       0.48      0.50      0.49     15011\n",
      "         2.0       0.43      0.73      0.54     17385\n",
      "\n",
      "    accuracy                           0.45     45428\n",
      "   macro avg       0.43      0.41      0.35     45428\n",
      "weighted avg       0.43      0.45      0.37     45428\n",
      "\n",
      "day_5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.02      0.04     13315\n",
      "         1.0       0.45      0.19      0.27     13586\n",
      "         2.0       0.42      0.89      0.57     18527\n",
      "\n",
      "    accuracy                           0.42     45428\n",
      "   macro avg       0.41      0.37      0.29     45428\n",
      "weighted avg       0.41      0.42      0.32     45428\n",
      "\n",
      "day_6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.01      0.02     13618\n",
      "         1.0       0.40      0.07      0.12     12280\n",
      "         2.0       0.43      0.96      0.60     19530\n",
      "\n",
      "    accuracy                           0.43     45428\n",
      "   macro avg       0.40      0.34      0.24     45428\n",
      "weighted avg       0.40      0.43      0.29     45428\n",
      "\n",
      "day_7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.01      0.01     13845\n",
      "         1.0       0.35      0.02      0.04     11424\n",
      "         2.0       0.45      0.98      0.61     20159\n",
      "\n",
      "    accuracy                           0.44     45428\n",
      "   macro avg       0.40      0.34      0.22     45428\n",
      "weighted avg       0.41      0.44      0.29     45428\n",
      "\n",
      "day_8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.01      0.01     14153\n",
      "         1.0       0.34      0.01      0.02     10523\n",
      "         2.0       0.46      0.99      0.62     20752\n",
      "\n",
      "    accuracy                           0.46     45428\n",
      "   macro avg       0.38      0.33      0.22     45428\n",
      "weighted avg       0.40      0.46      0.29     45428\n",
      "\n",
      "day_9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.37      0.01      0.01     14146\n",
      "         1.0       0.32      0.00      0.01     10071\n",
      "         2.0       0.47      0.99      0.63     21211\n",
      "\n",
      "    accuracy                           0.47     45428\n",
      "   macro avg       0.39      0.33      0.22     45428\n",
      "weighted avg       0.41      0.47      0.30     45428\n",
      "\n",
      "day_10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.00      0.01     14302\n",
      "         1.0       0.48      0.01      0.01      9479\n",
      "         2.0       0.48      0.99      0.64     21647\n",
      "\n",
      "    accuracy                           0.48     45428\n",
      "   macro avg       0.44      0.33      0.22     45428\n",
      "weighted avg       0.44      0.48      0.31     45428\n",
      "\n",
      "day_11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.00      0.00     14377\n",
      "         1.0       0.32      0.00      0.00      8973\n",
      "         2.0       0.49      1.00      0.65     22078\n",
      "\n",
      "    accuracy                           0.49     45428\n",
      "   macro avg       0.39      0.33      0.22     45428\n",
      "weighted avg       0.41      0.49      0.32     45428\n",
      "\n",
      "day_12\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.00      0.00     14463\n",
      "         1.0       0.41      0.00      0.00      8526\n",
      "         2.0       0.49      1.00      0.66     22439\n",
      "\n",
      "    accuracy                           0.49     45428\n",
      "   macro avg       0.40      0.33      0.22     45428\n",
      "weighted avg       0.42      0.49      0.33     45428\n",
      "\n",
      "day_13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.00      0.00     14435\n",
      "         1.0       0.40      0.00      0.00      8128\n",
      "         2.0       0.50      1.00      0.67     22865\n",
      "\n",
      "    accuracy                           0.50     45428\n",
      "   macro avg       0.42      0.33      0.22     45428\n",
      "weighted avg       0.44      0.50      0.34     45428\n",
      "\n",
      "day_14\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.00      0.00     14480\n",
      "         1.0       0.00      0.00      0.00      7775\n",
      "         2.0       0.51      1.00      0.68     23173\n",
      "\n",
      "    accuracy                           0.51     45428\n",
      "   macro avg       0.30      0.33      0.23     45428\n",
      "weighted avg       0.38      0.51      0.35     45428\n",
      "\n",
      "day_15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.00      0.00     14487\n",
      "         1.0       0.00      0.00      0.00      7488\n",
      "         2.0       0.52      1.00      0.68     23453\n",
      "\n",
      "    accuracy                           0.52     45428\n",
      "   macro avg       0.28      0.33      0.23     45428\n",
      "weighted avg       0.37      0.52      0.35     45428\n",
      "\n",
      "\n",
      " Completed fitting LReg \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Running all classifiers and storing the results\n",
    "less_columns = False\n",
    "minify = True\n",
    "print_data = False\n",
    "\n",
    "models = [\n",
    "            RandomForestClassifier,\n",
    "            KNeighborsClassifier,\n",
    "            DecisionTreeClassifier,\n",
    "            MLPClassifier,\n",
    "            LogisticRegression\n",
    "        ]\n",
    "\n",
    "model_names = ['RF', 'KNN', 'DTree', 'MLP', 'LReg']\n",
    "\n",
    "params = [{\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 10\n",
    "    },\n",
    "    {'weights' : 'uniform',\n",
    "    'n_neighbors' : 5,\n",
    "    'n_jobs': -1\n",
    "    },\n",
    "    {'random_state': 42,\n",
    "    'criterion' : 'gini', \n",
    "    'max_depth' : 3, \n",
    "    'min_samples_leaf' : 5\n",
    "    },\n",
    "    {'random_state': 42,\n",
    "    'solver' : 'lbfgs', \n",
    "    'alpha' : 1e-5, \n",
    "    'hidden_layer_sizes' : (5, 2), \n",
    "    'random_state' : 0\n",
    "    },\n",
    "    {\n",
    "    'random_state' : 42,\n",
    "    'penalty' : 'l2',\n",
    "    'C' : 0.8,\n",
    "    'n_jobs': -1\n",
    "    }]\n",
    "\n",
    "reports = {}\n",
    "counter = 0\n",
    "\n",
    "for m in models:\n",
    "    report = run_classifier(df_new, m, params[counter], model_names[counter],  minify, less_columns, print_data)\n",
    "    dataframe = pd.DataFrame.from_dict(report)\n",
    "    reports[model_names[counter]] = dataframe\n",
    "    print(\"\\n Completed fitting\", model_names[counter], '\\n')\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_weigthed</th>\n",
       "      <th>model</th>\n",
       "      <th>sell_recall</th>\n",
       "      <th>buy_precison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.566435</td>\n",
       "      <td>0.531608</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.518469</td>\n",
       "      <td>0.511131</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.547592</td>\n",
       "      <td>0.546958</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.564806</td>\n",
       "      <td>0.565081</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.563089</td>\n",
       "      <td>0.562792</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.589284</td>\n",
       "      <td>0.587561</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.617086</td>\n",
       "      <td>0.612837</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.654134</td>\n",
       "      <td>0.649125</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.670556</td>\n",
       "      <td>0.662363</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.691578</td>\n",
       "      <td>0.682342</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.705336</td>\n",
       "      <td>0.696017</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.729638</td>\n",
       "      <td>0.719227</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.735251</td>\n",
       "      <td>0.723136</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.757815</td>\n",
       "      <td>0.746166</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.763802</td>\n",
       "      <td>0.752039</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  accuracy  f1_weigthed model  sell_recall  buy_precison\n",
       "0    1  0.566435     0.531608    RF         0.21          0.31\n",
       "1    2  0.518469     0.511131    RF         0.38          0.48\n",
       "2    3  0.547592     0.546958    RF         0.49          0.57\n",
       "3    4  0.564806     0.565081    RF         0.54          0.61\n",
       "4    5  0.563089     0.562792    RF         0.56          0.63\n",
       "5    6  0.589284     0.587561    RF         0.61          0.67\n",
       "6    7  0.617086     0.612837    RF         0.65          0.69\n",
       "7    8  0.654134     0.649125    RF         0.69          0.73\n",
       "8    9  0.670556     0.662363    RF         0.71          0.74\n",
       "9   10  0.691578     0.682342    RF         0.74          0.75\n",
       "10  11  0.705336     0.696017    RF         0.74          0.77\n",
       "11  12  0.729638     0.719227    RF         0.78          0.79\n",
       "12  13  0.735251     0.723136    RF         0.78          0.79\n",
       "13  14  0.757815     0.746166    RF         0.80          0.82\n",
       "14  15  0.763802     0.752039    RF         0.81          0.82"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reports['RF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_weigthed</th>\n",
       "      <th>model</th>\n",
       "      <th>sell_recall</th>\n",
       "      <th>buy_precison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.558884</td>\n",
       "      <td>0.518031</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.469974</td>\n",
       "      <td>0.458219</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.470063</td>\n",
       "      <td>0.468397</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.487167</td>\n",
       "      <td>0.487791</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.511271</td>\n",
       "      <td>0.511738</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.533680</td>\n",
       "      <td>0.532937</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.554416</td>\n",
       "      <td>0.551560</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.574205</td>\n",
       "      <td>0.571160</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.594941</td>\n",
       "      <td>0.589509</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.610835</td>\n",
       "      <td>0.604609</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.625143</td>\n",
       "      <td>0.618669</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.637206</td>\n",
       "      <td>0.629708</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.651272</td>\n",
       "      <td>0.642316</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.662983</td>\n",
       "      <td>0.653215</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.672141</td>\n",
       "      <td>0.662748</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  accuracy  f1_weigthed model  sell_recall  buy_precison\n",
       "0    1  0.558884     0.518031   KNN         0.24          0.28\n",
       "1    2  0.469974     0.458219   KNN         0.38          0.43\n",
       "2    3  0.470063     0.468397   KNN         0.48          0.51\n",
       "3    4  0.487167     0.487791   KNN         0.53          0.55\n",
       "4    5  0.511271     0.511738   KNN         0.57          0.60\n",
       "5    6  0.533680     0.532937   KNN         0.60          0.63\n",
       "6    7  0.554416     0.551560   KNN         0.63          0.65\n",
       "7    8  0.574205     0.571160   KNN         0.63          0.67\n",
       "8    9  0.594941     0.589509   KNN         0.67          0.69\n",
       "9   10  0.610835     0.604609   KNN         0.68          0.71\n",
       "10  11  0.625143     0.618669   KNN         0.69          0.72\n",
       "11  12  0.637206     0.629708   KNN         0.70          0.73\n",
       "12  13  0.651272     0.642316   KNN         0.71          0.75\n",
       "13  14  0.662983     0.653215   KNN         0.72          0.75\n",
       "14  15  0.672141     0.662748   KNN         0.73          0.76"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports['KNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_weigthed</th>\n",
       "      <th>model</th>\n",
       "      <th>sell_recall</th>\n",
       "      <th>buy_precison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.609184</td>\n",
       "      <td>0.497970</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.474509</td>\n",
       "      <td>0.415255</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.450229</td>\n",
       "      <td>0.382792</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.438342</td>\n",
       "      <td>0.363150</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.433037</td>\n",
       "      <td>0.354511</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.442502</td>\n",
       "      <td>0.331680</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.450295</td>\n",
       "      <td>0.322372</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.456811</td>\n",
       "      <td>0.286483</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.466915</td>\n",
       "      <td>0.297235</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.476512</td>\n",
       "      <td>0.307568</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.484943</td>\n",
       "      <td>0.331120</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.493946</td>\n",
       "      <td>0.326629</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.503324</td>\n",
       "      <td>0.337033</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.510104</td>\n",
       "      <td>0.344620</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.516268</td>\n",
       "      <td>0.351563</td>\n",
       "      <td>DTree</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  accuracy  f1_weigthed  model  sell_recall  buy_precison\n",
       "0    1  0.609184     0.497970  DTree         0.00          0.36\n",
       "1    2  0.474509     0.415255  DTree         0.00          0.37\n",
       "2    3  0.450229     0.382792  DTree         0.00          0.40\n",
       "3    4  0.438342     0.363150  DTree         0.00          0.43\n",
       "4    5  0.433037     0.354511  DTree         0.00          0.44\n",
       "5    6  0.442502     0.331680  DTree         0.00          0.45\n",
       "6    7  0.450295     0.322372  DTree         0.00          0.45\n",
       "7    8  0.456811     0.286483  DTree         0.00          0.46\n",
       "8    9  0.466915     0.297235  DTree         0.00          0.47\n",
       "9   10  0.476512     0.307568  DTree         0.00          0.48\n",
       "10  11  0.484943     0.331120  DTree         0.02          0.49\n",
       "11  12  0.493946     0.326629  DTree         0.00          0.49\n",
       "12  13  0.503324     0.337033  DTree         0.00          0.50\n",
       "13  14  0.510104     0.344620  DTree         0.00          0.51\n",
       "14  15  0.516268     0.351563  DTree         0.00          0.52"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports['DTree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_weigthed</th>\n",
       "      <th>model</th>\n",
       "      <th>sell_recall</th>\n",
       "      <th>buy_precison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.606674</td>\n",
       "      <td>0.458156</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.489456</td>\n",
       "      <td>0.429397</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.482500</td>\n",
       "      <td>0.410603</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.382693</td>\n",
       "      <td>0.211839</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.407832</td>\n",
       "      <td>0.236288</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.429911</td>\n",
       "      <td>0.258511</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.443757</td>\n",
       "      <td>0.272789</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.456811</td>\n",
       "      <td>0.286483</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.466915</td>\n",
       "      <td>0.297235</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.476512</td>\n",
       "      <td>0.307568</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.317895</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.493946</td>\n",
       "      <td>0.326629</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.503324</td>\n",
       "      <td>0.337033</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.510104</td>\n",
       "      <td>0.344620</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.516268</td>\n",
       "      <td>0.351563</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  accuracy  f1_weigthed model  sell_recall  buy_precison\n",
       "0    1  0.606674     0.458156   MLP          0.0          0.00\n",
       "1    2  0.489456     0.429397   MLP          0.0          0.38\n",
       "2    3  0.482500     0.410603   MLP          0.0          0.43\n",
       "3    4  0.382693     0.211839   MLP          0.0          0.38\n",
       "4    5  0.407832     0.236288   MLP          0.0          0.41\n",
       "5    6  0.429911     0.258511   MLP          0.0          0.43\n",
       "6    7  0.443757     0.272789   MLP          0.0          0.44\n",
       "7    8  0.456811     0.286483   MLP          0.0          0.46\n",
       "8    9  0.466915     0.297235   MLP          0.0          0.47\n",
       "9   10  0.476512     0.307568   MLP          0.0          0.48\n",
       "10  11  0.486000     0.317895   MLP          0.0          0.49\n",
       "11  12  0.493946     0.326629   MLP          0.0          0.49\n",
       "12  13  0.503324     0.337033   MLP          0.0          0.50\n",
       "13  14  0.510104     0.344620   MLP          0.0          0.51\n",
       "14  15  0.516268     0.351563   MLP          0.0          0.52"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports['MLP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_weigthed</th>\n",
       "      <th>model</th>\n",
       "      <th>sell_recall</th>\n",
       "      <th>buy_precison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.606454</td>\n",
       "      <td>0.481691</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.484327</td>\n",
       "      <td>0.378652</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.453267</td>\n",
       "      <td>0.375862</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.445738</td>\n",
       "      <td>0.371359</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.424474</td>\n",
       "      <td>0.324250</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.431958</td>\n",
       "      <td>0.292732</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.443361</td>\n",
       "      <td>0.285587</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.455600</td>\n",
       "      <td>0.293561</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.465880</td>\n",
       "      <td>0.301569</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.476006</td>\n",
       "      <td>0.311885</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.485560</td>\n",
       "      <td>0.319487</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.493484</td>\n",
       "      <td>0.327731</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.502906</td>\n",
       "      <td>0.338044</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.509884</td>\n",
       "      <td>0.345915</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.516003</td>\n",
       "      <td>0.352091</td>\n",
       "      <td>LReg</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  accuracy  f1_weigthed model  sell_recall  buy_precison\n",
       "0    1  0.606454     0.481691  LReg         0.01          0.35\n",
       "1    2  0.484327     0.378652  LReg         0.01          0.41\n",
       "2    3  0.453267     0.375862  LReg         0.01          0.44\n",
       "3    4  0.445738     0.371359  LReg         0.01          0.43\n",
       "4    5  0.424474     0.324250  LReg         0.02          0.42\n",
       "5    6  0.431958     0.292732  LReg         0.01          0.43\n",
       "6    7  0.443361     0.285587  LReg         0.01          0.45\n",
       "7    8  0.455600     0.293561  LReg         0.01          0.46\n",
       "8    9  0.465880     0.301569  LReg         0.01          0.47\n",
       "9   10  0.476006     0.311885  LReg         0.00          0.48\n",
       "10  11  0.485560     0.319487  LReg         0.00          0.49\n",
       "11  12  0.493484     0.327731  LReg         0.00          0.49\n",
       "12  13  0.502906     0.338044  LReg         0.00          0.50\n",
       "13  14  0.509884     0.345915  LReg         0.00          0.51\n",
       "14  15  0.516003     0.352091  LReg         0.00          0.52"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports['LReg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, we can see that two models stand out form the rest: Random Forests and KNNeighbors, with F1 Weighted scores for the 15th day of 0.75 and 0.66, and accuracy of 76 and 67 respectively. This is a significant improvement from 0.33 if the classes were to be randomly guessed. We also see a clear trend: the scores improve with the number of days. Hence, for purposes of this paper we decided to fine-tune the best two models only on the 15th day labels, since this is where we are most likely to get the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline with gridsearch\n",
    "def run_classifiers_gs(DF, model_list, model_names, grids):\n",
    "    \n",
    "    df = DF\n",
    "\n",
    "    #Dropping N/As and selecting fatures/labels\n",
    "    df.dropna(inplace=True)\n",
    "    features = df.iloc[:,1:23]\n",
    "    labels = df.iloc[:,23:]\n",
    "\n",
    "    #Storage for test set scores, confusion matrices and best models\n",
    "    results = pd.DataFrame(index=list(range(0, (len(models)))), columns=['Model Name', 'Accuracy', 'F1 Weighted',\n",
    "    'Sell Recall', 'Buy Precison'])\n",
    "    report_data = {}\n",
    "    best_models = {}\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    for m in model_list:\n",
    "\n",
    "        # specify the feature set, target set, the test size and random_state to select records randomly\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels.iloc[:,14], test_size=0.3,random_state=42) \n",
    "\n",
    "        # Scaling values in the feature set\n",
    "        scaling = MinMaxScaler(feature_range=(0,1)).fit(X_train)\n",
    "        X_train = scaling.transform(X_train)\n",
    "        X_test = scaling.transform(X_test)\n",
    "\n",
    "        #Use the grid corresponfing to the classifier\n",
    "        random_gridcv = grids[counter]\n",
    "\n",
    "        clf = RandomizedSearchCV(m(), random_gridcv, cv = 10, scoring= 'f1_weighted')\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "\n",
    "        #Output best accuracy and parameters found with the gridseacrh\n",
    "        print(f'Best training f1 weighted score is {np.abs(clf.best_score_)}')\n",
    "        print(f'Best set of parameters is  {clf.best_params_}')\n",
    "\n",
    "        #Fitting and storing the best models\n",
    "        params = clf.best_params_\n",
    "        best = m().set_params(**params)\n",
    "        best = best.fit(X_train, y_train)\n",
    "        best_models[model_names[counter]] = best\n",
    "\n",
    "        #Predicting y using the best model and X_test\n",
    "        y_pred = best.predict(X_test)\n",
    "\n",
    "        #Calculating and storing test scores\n",
    "        accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "        f1_w = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "        # f1_scores =[] \n",
    "        # f1_scores.insert(2, metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        # unravel report for the given day       \n",
    "        lines = report.split('\\n')\n",
    "        for line in lines[2:-5]:\n",
    "\n",
    "            row_data = line.split('     ')\n",
    "\n",
    "            # update recall for sell\n",
    "            if(float(row_data[1])==0.0):\n",
    "                results.loc[counter, 'Sell Recall']= float(row_data[3])\n",
    "            # update precison for buy\n",
    "            if(float(row_data[1])==2.0):\n",
    "                results.loc[counter, 'Buy Precison']= float(row_data[2])\n",
    "\n",
    "\n",
    "        results.loc[counter, 'Model Name'] = model_names[counter]\n",
    "        results.loc[counter, 'Accuracy'] = accuracy\n",
    "        results.loc[counter, 'F1 Weighted'] = f1_w\n",
    "        \n",
    "        #Calculating and storing confusion matrices\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        report_data[model_names[counter]] = report\n",
    "\n",
    "        print(\"\\n Completed fitting\", model_names[counter], '\\n')\n",
    "\n",
    "        counter += 1\n",
    "    \n",
    "    return results, report_data, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training f1 weighted score is 0.7792387480627171\n",
      "Best set of parameters is  {'random_state': 42, 'n_jobs': -1, 'n_estimators': 50, 'criterion': 'gini'}\n",
      "\n",
      " Completed fitting RF \n",
      "\n",
      "Best training f1 weighted score is 0.6984272669440001\n",
      "Best set of parameters is  {'weights': 'distance', 'n_neighbors': 3, 'n_jobs': -1}\n",
      "\n",
      " Completed fitting KNN \n",
      "\n"
     ]
    }
   ],
   "source": [
    "RF = RandomForestClassifier\n",
    "KNN = KNeighborsClassifier\n",
    "\n",
    "models = [RF, KNN]\n",
    "model_names = ['RF', 'KNN']\n",
    "\n",
    "\n",
    "grids = [{\n",
    " 'random_state': [42],\n",
    " 'n_jobs':[-1],\n",
    " 'n_estimators': [5,10,20,40,50],\n",
    " 'criterion' : ['gini','entropy']\n",
    " },\n",
    " {\n",
    " 'n_neighbors': [3,4,5,6],\n",
    " 'weights':['distance', 'uniform'],\n",
    " 'n_jobs':[-1]\n",
    " }]\n",
    "\n",
    "test_scores, confusion_mx, best_est_models = run_classifiers_gs(df_new, models, model_names, grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Sell Recall</th>\n",
       "      <th>Buy Precison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.79713</td>\n",
       "      <td>0.784225</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.708132</td>\n",
       "      <td>0.704309</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Name  Accuracy F1 Weighted Sell Recall Buy Precison\n",
       "0         RF   0.79713    0.784225        0.83         0.83\n",
       "1        KNN  0.708132    0.704309        0.72         0.79"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42),\n",
       " 'KNN': KNeighborsClassifier(n_jobs=-1, n_neighbors=3, weights='distance')}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.83      0.83     14674\n",
      "         1.0       0.53      0.34      0.42      7299\n",
      "         2.0       0.83      0.92      0.87     23455\n",
      "\n",
      "    accuracy                           0.80     45428\n",
      "   macro avg       0.73      0.70      0.70     45428\n",
      "weighted avg       0.78      0.80      0.78     45428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_scores\n",
    "best_est_models\n",
    "print(confusion_mx['RF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7514720384087623"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "df = df_new\n",
    "\n",
    "#Dropping N/As and selecting fatures/labels\n",
    "df.dropna(inplace=True)\n",
    "features = df.iloc[:,1:23]\n",
    "labels = df.iloc[:,23:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels.iloc[:,14], test_size=0.3,random_state=42)\n",
    "scaling = MinMaxScaler(feature_range=(0,1)).fit(X_train)\n",
    "X_train = scaling.transform(X_train)\n",
    "X_test = scaling.transform(X_test)\n",
    "\n",
    "\n",
    "vote = VotingClassifier(estimators=[\n",
    "    ('RF', best_est_models['RF']),\n",
    "    ('KNN', best_est_models['KNN'])],\n",
    "    voting = 'hard',\n",
    "    weights = [1,1]\n",
    "        )\n",
    "\n",
    "vote = vote.fit(X_train, y_train)\n",
    "\n",
    "y_pred = vote.predict(X_test)\n",
    "f1_weighted =  metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "f1_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting does not result in improvemnt. Hence the best classifier found is Random Forest with F1 score of 0.78."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fff61027b577cb78411ef4490657ef0da7090231261930ff04d84f34de6b36c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
